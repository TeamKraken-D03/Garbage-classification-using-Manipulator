{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel coordinates extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle click event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_coordinates = []\n",
    "\n",
    "# Track mouse clicks\n",
    "def click_event(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        pixel_coordinates.append((x, y))\n",
    "        print(f\"Clicked at: ({x}, {y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(0)\n",
    "pixel_coordinates = []\n",
    "\n",
    "# Track mouse clicks\n",
    "def click_event(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        pixel_coordinates.append((x, y))\n",
    "        print(f\"Clicked at: ({x}, {y})\")\n",
    "\n",
    "cv2.namedWindow(\"Pixel Coordinates\")\n",
    "cv2.setMouseCallback(\"Pixel Coordinates\", click_event)\n",
    "\n",
    "# Display video feed\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Pixel Coordinates\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate homgeneous transform matrix for transformation from camera's frame of reference to end effector frame of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homography Matrix: [[-1.12652760e+00  3.40585199e-02  6.19708826e+02]\n",
      " [ 2.72743826e-02  1.15644498e+00 -9.04650194e+01]\n",
      " [ 6.52525716e-05  5.57229232e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "pixel_coordinates = [[197,32], [328,43], [256,83], [402,77], [231,137], [331,153], [316,166], [260,224], [405,220], [245,265], [333,260]]\n",
    "robot_coordinates = [[393.6,-47.6], [245.8,-29.9], [325.3,11.4], [164.9,7.9], [356.9,73.4], [244.5,92.9], [262.6,107.8], [323.9,170.4], [164.6,168.5], [343.1,215.1], [243.9,212.2]]\n",
    "pixel_points = np.array(pixel_coordinates, dtype=np.float32)\n",
    "robot_points = np.array(robot_coordinates, dtype=np.float32)\n",
    "\n",
    "homography_matrix, _ = cv2.findHomography(pixel_points, robot_points)\n",
    "print(\"Homography Matrix:\", homography_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_robot(x, y, matrix):\n",
    "    pixel = np.array([x, y, 1]).reshape(3, 1)\n",
    "    robot_coords = np.dot(matrix, pixel)\n",
    "    robot_coords /= robot_coords[2]  \n",
    "    return robot_coords[0][0], robot_coords[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box coordinates from camera reference to end effector reference frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = pixel_to_robot(306, 303, homography_matrix)\n",
    "box_x, box_y = box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK_VERSION: 1.14.8\n",
      "ROBOT_IP: 192.168.1.155, VERSION: v2.2.0, PROTOCOL: V1, DETAIL: 6,9,LI1006,DL1000,v2.2.0, TYPE1300: [0, 0]\n",
      "change protocol identifier to 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from xarm.wrapper import XArmAPI\n",
    "\n",
    "\n",
    "arm = XArmAPI('192.168.1.155')\n",
    "arm.motion_enable(enable=True)\n",
    "arm.set_mode(0)\n",
    "arm.set_state(0)\n",
    "arm.connect()\n",
    "arm.move_gohome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start window listener stream\n",
    "1. Click object to pick\n",
    "2. Transform pixel to end effector coordinate frame (P)\n",
    "3. Move arm to the point (P) with a fixed height\n",
    "4. Pick the object\n",
    "5. Go to dropping destination\n",
    "6. Lower arm and drop the object\n",
    "7. Go back to home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick up and drop sub-routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_coordinates = [] # Reinitialize pixel coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def pick_up_and_drop(x_robot, y_robot):\n",
    "    arm.set_position(x_robot, y_robot, 70)  \n",
    "    arm.set_position(x_robot, y_robot, 17, wait=True)  \n",
    "    arm.set_suction_cup(False)\n",
    "    time.sleep(1)\n",
    "    arm.set_position(x_robot, y_robot, 16.5, wait=True)  \n",
    "    arm.set_position(x_robot, y_robot, 70, wait=True)  \n",
    "    arm.set_position(box_x, box_y, 200)\n",
    "    arm.set_position(box_x, box_y, 100)\n",
    "    arm.set_suction_cup(True)\n",
    "    arm.set_position(box_x, box_y, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pick up routine\n",
    "1. Start stream\n",
    "2. User selects object\n",
    "3. Arm moves and places it's end effector over the center of the object\n",
    "4. Arm lowers and picks up the object\n",
    "5. Arm goes to box location\n",
    "6. Arm drops the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked at: (257, 157)\n",
      "Moving to: (327.1890762621443, 95.66513696902115)\n",
      "Clicked at: (260, 165)\n",
      "Moving to: (323.956617351429, 104.70077500758353)\n",
      "Clicked at: (239, 201)\n",
      "Moving to: (347.9898703731883, 144.62370994077784)\n",
      "Clicked at: (232, 122)\n",
      "Moving to: (354.7279688698958, 55.72646526077061)\n",
      "Clicked at: (252, 151)\n",
      "Moving to: (332.6965994095523, 88.82336310257618)\n",
      "Clicked at: (248, 144)\n",
      "Moving to: (337.07492422482045, 80.86951765030584)\n",
      "Clicked at: (176, 96)\n",
      "Moving to: (417.6784492829194, 24.934250731439587)\n",
      "Clicked at: (276, 95)\n",
      "Moving to: (304.9171657381117, 26.311828789104666)\n",
      "Clicked at: (196, 185)\n",
      "Moving to: (396.06190836446785, 125.91467334432974)\n",
      "Clicked at: (348, 147)\n",
      "Moving to: (225.7095868363908, 86.35556337292725)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "cv2.namedWindow(\"Click to Move\")\n",
    "cv2.setMouseCallback(\"Click to Move\", click_event)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Click to Move\", frame)\n",
    "    \n",
    "    if len(pixel_coordinates) > 0:\n",
    "        x_pixel, y_pixel = pixel_coordinates.pop(0)\n",
    "        x_robot, y_robot = pixel_to_robot(x_pixel, y_pixel, homography_matrix)\n",
    "        print(f\"Moving to: ({x_robot}, {y_robot})\")\n",
    "        pick_up_and_drop(x_robot, y_robot)\n",
    "        # arm.set_position(x_robot, y_robot, 70, speed=100)  \n",
    "        # arm.set_position(x_robot, y_robot, 27, speed=100)  \n",
    "        # arm.set_suction_cup(False)\n",
    "        # arm.set_position(box_x, box_y, 200, speed=100)\n",
    "        # arm.set_position(box_x, box_y, 100, speed=100)\n",
    "        # arm.set_suction_cup(True)\n",
    "        # arm.set_position(box_x, box_y, 200, speed=100)\n",
    "        # arm.move_gohome()\n",
    "        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as captured_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame. Exiting...\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Webcam Preview - Press Q to capture', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.imwrite('captured_image.jpg', frame)\n",
    "        print(\"Image saved as captured_image.jpg\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "model_name = \"gemini-2.0-flash\" \n",
    "\n",
    "bounding_box_system_instructions = \"\"\"\n",
    "    Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.\n",
    "    If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..).\n",
    "    \"\"\"\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def plot_bounding_boxes(im, bounding_boxes):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    'pink',\n",
    "    'purple',\n",
    "    'brown',\n",
    "    'gray',\n",
    "    'beige',\n",
    "    'turquoise',\n",
    "    'cyan',\n",
    "    'magenta',\n",
    "    'lime',\n",
    "    'navy',\n",
    "    'maroon',\n",
    "    'teal',\n",
    "    'olive',\n",
    "    'coral',\n",
    "    'lavender',\n",
    "    'violet',\n",
    "    'gold',\n",
    "    'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Parsing out the markdown fencing\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over the bounding boxes\n",
    "    for i, bounding_box in enumerate(json.loads(bounding_boxes)):\n",
    "      # Select a color from the list\n",
    "      color = colors[i % len(colors)]\n",
    "\n",
    "      # Convert normalized coordinates to absolute coordinates\n",
    "      abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n",
    "      abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n",
    "      abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n",
    "      abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n",
    "\n",
    "      if abs_x1 > abs_x2:\n",
    "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "      if abs_y1 > abs_y2:\n",
    "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "      # Draw the bounding box\n",
    "      draw.rectangle(\n",
    "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "      )\n",
    "\n",
    "      # Draw the text\n",
    "      if \"label\" in bounding_box:\n",
    "        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "\n",
    "    # Display the image\n",
    "    img.save(\"output_image.jpg\")  # or \"output_image.png\"\n",
    "    print(\"Image saved as output_image.jpg\")\n",
    "\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(json_output):\n",
    "    # Parsing out the markdown fencing\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
    "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
    "            break  # Exit the loop once \"```json\" is found\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptured_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load and resize image\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28mopen\u001b[39m(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m      6\u001b[0m im\u001b[38;5;241m.\u001b[39mthumbnail([\u001b[38;5;241m1024\u001b[39m,\u001b[38;5;241m1024\u001b[39m], Image\u001b[38;5;241m.\u001b[39mResampling\u001b[38;5;241m.\u001b[39mLANCZOS)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run model to find bounding boxes\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"Detect the 2d bounding boxes of the small cubes (with “label” as topping description”)\"  # @param {type:\"string\"}\n",
    "\n",
    "image = \"captured_image.jpg\"\n",
    "# Load and resize image\n",
    "im = Image.open(io.BytesIO(open(image, \"rb\").read()))\n",
    "im.thumbnail([1024,1024], Image.Resampling.LANCZOS)\n",
    "\n",
    "# Run model to find bounding boxes\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=[prompt, im],\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=bounding_box_system_instructions,\n",
    "        temperature=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check output\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480)\n",
      "Image saved as output_image.jpg\n"
     ]
    }
   ],
   "source": [
    "plot_bounding_boxes(im, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBOT_IP: 192.168.1.155, VERSION: v2.2.0, PROTOCOL: V1, DETAIL: 6,9,LI1006,DL1000,v2.2.0, TYPE1300: [0, 0]\n",
      "change protocol identifier to 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from xarm.wrapper import XArmAPI\n",
    "\n",
    "\n",
    "arm = XArmAPI('192.168.1.155')\n",
    "arm.motion_enable(enable=True)\n",
    "arm.set_mode(0)\n",
    "arm.set_state(0)\n",
    "arm.connect()\n",
    "arm.move_gohome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = parse_json(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'box_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m c2 = (abs_y1 + abs_y2) / \u001b[32m2\u001b[39m\n\u001b[32m     19\u001b[39m c1_robot, c2_robot = pixel_to_robot(c1, c2, homography_matrix)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpick_up_and_drop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc1_robot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2_robot\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mpick_up_and_drop\u001b[39m\u001b[34m(x_robot, y_robot)\u001b[39m\n\u001b[32m      7\u001b[39m arm.set_position(x_robot, y_robot, \u001b[32m16.5\u001b[39m, wait=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m      8\u001b[39m arm.set_position(x_robot, y_robot, \u001b[32m50\u001b[39m, wait=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m arm.set_position(\u001b[43mbox_x\u001b[49m, box_y, \u001b[32m200\u001b[39m)\n\u001b[32m     10\u001b[39m arm.set_position(box_x, box_y, \u001b[32m100\u001b[39m)\n\u001b[32m     11\u001b[39m arm.set_suction_cup(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'box_x' is not defined"
     ]
    }
   ],
   "source": [
    "for i, bounding_box in enumerate(json.loads(bounding_boxes)):\n",
    "\n",
    "    width, height = im.size\n",
    "    # Convert normalized coordinates to absolute coordinates\n",
    "    abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n",
    "    abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n",
    "    abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n",
    "    abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n",
    "\n",
    "    if abs_x1 > abs_x2:\n",
    "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "    if abs_y1 > abs_y2:\n",
    "        abs_y1, abs_y2 = abs_y2, abs_y\n",
    "    \n",
    "    c1 = (abs_x1 + abs_x2) / 2\n",
    "    c2 = (abs_y1 + abs_y2) / 2\n",
    "\n",
    "    c1_robot, c2_robot = pixel_to_robot(c1, c2, homography_matrix)\n",
    "    pick_up_and_drop(c1_robot, c2_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get which bounding box to pick from llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: ChatCompletion(id='chatcmpl-89c7db2e-34b2-4900-b69e-342084abef90', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n    \"centroids\": [\\n        {\\n            \"centroid\": [\\n                299.5,\\n                509.0\\n            ]\\n        }\\n    ]\\n}', role='assistant', function_call=None, reasoning=None, tool_calls=None))], created=1742563057, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_dadc9d6142', usage=CompletionUsage(completion_tokens=34, prompt_tokens=425, total_tokens=459, completion_time=0.028333333, prompt_time=0.080756094, queue_time=0.23951559700000002, total_time=0.109089427), x_groq={'id': 'req_01jpwdm7bef5tbrkg4jzvw1gz4'})\n",
      "Groq API response: {\n",
      "    \"centroids\": [\n",
      "        {\n",
      "            \"centroid\": [\n",
      "                299.5,\n",
      "                509.0\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Returned centroids: [{'centroid': [299.5, 509.0]}]\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "def pick_and_place_all_blocks(centroids, homography_matrix, drop_location=None):\n",
    "    \"\"\"\n",
    "    Pick and place all blocks at the specified centroids\n",
    "    \n",
    "    Args:\n",
    "        centroids: List of dictionaries, each with a 'centroid' key containing [x, y] coordinates\n",
    "        homography_matrix: 3x3 homography matrix that maps pixel coordinates to robot coordinates\n",
    "        drop_location: Optional tuple (x, y) for the drop location in robot coordinates\n",
    "                      If None, defaults to (250, 0)\n",
    "    \"\"\"\n",
    "    # Default drop location if not specified\n",
    "    if drop_location is None:\n",
    "        drop_location = (250, 0)\n",
    "    \n",
    "    box_x, box_y = drop_location\n",
    "    \n",
    "    # Initialize the XArm\n",
    "    arm = XArmAPI('192.168.1.155')\n",
    "    arm.motion_enable(enable=True)\n",
    "    arm.set_mode(0)\n",
    "    arm.set_state(0)\n",
    "    arm.connect()\n",
    "    \n",
    "    # Move to home position to start\n",
    "    print(\"Moving to home position...\")\n",
    "    arm.move_gohome(wait=True)\n",
    "    \n",
    "    # Process each centroid\n",
    "    for i, centroid_obj in enumerate(centroids):\n",
    "        print(f\"Processing block {i+1}/{len(centroids)}\")\n",
    "        \n",
    "        # Extract the centroid coordinates\n",
    "        pixel_x, pixel_y = centroid_obj['centroid']\n",
    "        \n",
    "        # Convert pixel coordinates to robot coordinates\n",
    "        x_robot, y_robot = pixel_to_robot(pixel_x, pixel_y, homography_matrix)\n",
    "        print(f\"Pixel coordinates: ({pixel_x}, {pixel_y}) -> Robot coordinates: ({x_robot}, {y_robot})\")\n",
    "        \n",
    "        # Pick up the block\n",
    "        print(f\"Picking up block at ({x_robot}, {y_robot})...\")\n",
    "        \n",
    "        # Move above the block\n",
    "        arm.set_position(x=x_robot, y=y_robot, z=70, wait=True)\n",
    "        \n",
    "        # Lower to the block\n",
    "        arm.set_position(z=17, wait=True)\n",
    "        \n",
    "        # Turn off suction (to create suction)\n",
    "        arm.set_suction_cup(False)\n",
    "        time.sleep(1)  # Wait for suction to establish\n",
    "        \n",
    "        # Fine adjustment for better grip\n",
    "        arm.set_position(z=16.5, wait=True)\n",
    "        \n",
    "        # Lift the block\n",
    "        arm.set_position(z=70, wait=True)\n",
    "        \n",
    "        # Move to the drop location\n",
    "        print(f\"Moving to drop location ({box_x}, {box_y})...\")\n",
    "        arm.set_position(x=box_x, y=box_y, z=200, wait=True)\n",
    "        \n",
    "        # Lower to drop height\n",
    "        arm.set_position(z=100, wait=True)\n",
    "        \n",
    "        # Release the block\n",
    "        arm.set_suction_cup(True)  # Turn on suction (to release)\n",
    "        time.sleep(0.5)  # Wait for release\n",
    "        \n",
    "        # Move up\n",
    "        arm.set_position(z=200, wait=True)\n",
    "        \n",
    "        print(f\"Block {i+1} placed successfully\")\n",
    "    \n",
    "    # Return to home position\n",
    "    print(\"All blocks processed. Returning to home position...\")\n",
    "    arm.move_gohome(wait=True)\n",
    "    \n",
    "    print(\"Operation completed successfully\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Your Groq API client setup\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")  # Make sure this is set in your .env file\n",
    ")\n",
    "\n",
    "# Sample data with object bounding boxes and labels\n",
    "data = [\n",
    "    {\"box_2d\": [237, 390, 300, 436], \"label\": \"green cube\"},\n",
    "    {\"box_2d\": [193, 488, 262, 535], \"label\": \"green cube\"},\n",
    "    {\"box_2d\": [368, 415, 425, 458], \"label\": \"red cube\"},\n",
    "    {\"box_2d\": [270, 548, 331, 593], \"label\": \"green cube\"},\n",
    "    {\"box_2d\": [275, 301, 343, 355], \"label\": \"blue cube\"},\n",
    "    {\"box_2d\": [268, 488, 331, 530], \"label\": \"yellow cube\"},\n",
    "    {\"box_2d\": [335, 470, 402, 520], \"label\": \"green cube\"},\n",
    "    {\"box_2d\": [231, 435, 306, 490], \"label\": \"blue cube\"},\n",
    "    {\"box_2d\": [295, 403, 362, 455], \"label\": \"red cube\"},\n",
    "    {\"box_2d\": [418, 498, 487, 548], \"label\": \"green cube\"}\n",
    "]\n",
    "\n",
    "# Calculate centroids from the bounding boxes\n",
    "objects = []\n",
    "for item in data:\n",
    "    x1, y1, x2, y2 = item[\"box_2d\"]\n",
    "    centroid_x = (x1 + x2) / 2\n",
    "    centroid_y = (y1 + y2) / 2\n",
    "    objects.append({\n",
    "        \"label\": item[\"label\"],\n",
    "        \"centroid\": [centroid_x, centroid_y]\n",
    "    })\n",
    "\n",
    "# User command\n",
    "custom_prompt = \"pick the yellow block\"\n",
    "\n",
    "# Send the objects and custom prompt to the Groq API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an assistant tasked with processing object data and returning only the centroids that match the user's request in JSON format.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"\n",
    "Here is a list of objects with their labels and centroids:\n",
    "{json.dumps(objects, indent=2)}\n",
    "\n",
    "The command is: \"{custom_prompt}\"\n",
    "\n",
    "Please return only the centroids of objects that match this command in this exact JSON format:\n",
    "{{\n",
    "    \"centroids\": [\n",
    "        {{\n",
    "            \"centroid\": [x, y]\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "Do not include any explanation, only the JSON.\n",
    "\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "# Debugging: Print the raw response\n",
    "print(\"Raw response:\", response)\n",
    "\n",
    "# Handle the response from the API\n",
    "try:\n",
    "    # Check if the response contains content\n",
    "    if response and hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "        # Access the message content\n",
    "        response_content = response.choices[0].message.content\n",
    "        print(\"Groq API response:\", response_content)\n",
    "\n",
    "        # Try parsing the response content as JSON\n",
    "        try:\n",
    "            result = json.loads(response_content)\n",
    "            # Assuming the Groq API returns the centroid values\n",
    "            centroids_from_response = result.get('centroids', [])\n",
    "            \n",
    "            if centroids_from_response:\n",
    "                print(\"Returned centroids:\", centroids_from_response)\n",
    "                \n",
    "                # Here you would typically use the centroids to control the robotic arm\n",
    "                # For example:\n",
    "                # for centroid_obj in centroids_from_response:\n",
    "                #     centroid = centroid_obj['centroid']\n",
    "                #     # Move arm to this centroid\n",
    "                #     # arm.set_position(x=centroid[0], y=centroid[1], z=some_z_value)\n",
    "            else:\n",
    "                print(\"No matching centroids found.\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Response is not valid JSON: {e}\")\n",
    "            print(\"Raw content:\", response_content)\n",
    "            \n",
    "    else:\n",
    "        print(\"No valid response or content in response.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing response: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
